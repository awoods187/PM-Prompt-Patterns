# Code Review & Refactoring for Open Source (Claude Optimized)

**Provider:** Claude (Anthropic)
**Optimizations:** XML tags, chain-of-thought reasoning, prompt caching

**Complexity**: üî¥ Advanced

## Claude-Specific Features

This variant is optimized for Claude models with:
- **XML structure** for clear parsing and better accuracy
- **Chain-of-thought** reasoning with `<thinking>` tags
- **Prompt caching** - category definitions cached for 90% cost savings

## Usage

```python
from ai_models import get_prompt
from pm_prompt_toolkit.providers import get_provider

# Load Claude-optimized prompt
prompt = get_prompt("developing-internal-tools/code-review-refactoring", provider="claude")

# Use with caching for cost savings
provider = get_provider("claude-sonnet-4-5", enable_caching=True)
result = provider.generate(prompt)
```

---

## Original Prompt (Enhanced with XML)

<task>
# Code Review & Refactoring for Open Source

**Complexity**: üî¥ Advanced
**Category**: Technical Documentation
**Model Compatibility**: ‚úÖ Claude (all) | ‚úÖ GPT-4 | ‚ö†Ô∏è Gemini (large context needed)

## Overview

Comprehensive code review and refactoring prompt for preparing internal codebases for team sharing or open-sourcing. Focuses on security audit, code quality, documentation, and best practices compliance.

**Business Value**:
- Prevent security incidents from exposed credentials
- Reduce onboarding time with better documentation
- Improve code maintainability and collaboration
- Ensure compliance with industry best practices
- Enable safe open-sourcing of internal projects

**Use Cases**:
- Pre-open-source security audit
- Internal code sharing preparation
- Legacy codebase modernization
- Team handoff preparation
- Technical debt reduction

**Production metrics**:
- Security issue detection: ~98% (credentials, keys)
- Documentation coverage improvement: 0% ‚Üí 90%+
- PEP 8 compliance improvement: Variable ‚Üí 95%+
- Processing time: ~2-5min per 1000 lines of code

---

## Base Prompt (Model Agnostic)

**Complexity**: üî¥ Advanced

```
You are a Staff Software Engineer preparing this codebase for internal team
sharing and potential open-sourcing. Conduct a comprehensive code review and
refactoring with these priorities:

## 1. SECURITY AUDIT (FIRST PRIORITY)
- Identify and flag ALL hardcoded credentials, API keys, tokens, or secrets
- Move sensitive configuration to environment variables
- Create .env.example with sanitized examples
- Validate .gitignore excludes all sensitive files
- Review authentication/authorization implementation
- Validate and sanitize external inputs

## 2. CODE QUALITY & DOCUMENTATION
- Add docstrings to all functions/classes (Google Style Guide format)
- Add type hints to all function signatures
- Comment complex logic and non-obvious design decisions
- Flag unclear code with # TODO: [REVIEW]
- Follow PEP 8 naming conventions
- Remove ALL commented-out code

## 3. CODEBASE STRUCTURE
Reorganize following Python best practices:
- Standard project layout (models/, utils/, config/, tests/)
- Proper __init__.py files
- Separation of concerns
- DRY principle throughout

Make code more Pythonic:
- Use comprehensions over verbose loops
- Context managers for resource management
- Proper exception handling patterns

## 4. LEGACY CLEANUP
Remove ALL:
- v1/v2 or deprecated version references
- Migration scripts for old versions
- Personal TODOs and single-developer notes
- "Temporary" fixes and workarounds
- Dead/unreachable code

## 5. DOCUMENTATION
Create/update README.md with:
- Project purpose and description
- Installation steps (detailed)
- Configuration requirements
- Usage examples with expected outputs
- Testing instructions
- System requirements
- Contributing guidelines

## OUTPUT FORMAT
For each modified file, provide:
1. **Security Issues:** [CRITICAL/HIGH/MEDIUM/LOW] + description
2. **Major Changes:** Summary of key modifications
3. **Breaking Changes:** List any breaking changes
4. **Discussion Items:** Tag architectural concerns as [NEEDS DISCUSSION]

## QUALITY CHECKLIST
Before completion, verify:
- [ ] Zero hardcoded secrets
- [ ] All functions documented with type hints
- [ ] Python conventions followed throughout
- [ ] No legacy version references
- [ ] README complete and tested
- [ ] PEP 8 compliant
- [ ] All TODOs resolved or tagged for review

**Priority Order:** Security ‚Üí Organization ‚Üí Documentation

**Note:** Flag questionable architectural decisions for team discussion
rather than making assumptions about intent.

---

CODEBASE TO REVIEW:

{paste_code_here}
```

**Performance**: Detects 95%+ of security issues, comprehensive refactoring suggestions.

---

## Model-Specific Optimizations

### Claude (Anthropic) - Comprehensive Analysis

**Complexity**: üî¥ Advanced

Claude excels at large codebase analysis with its 200K context window and strong reasoning.

```xml
<role>
You are a Staff Software Engineer conducting a comprehensive code review to prepare
this codebase for team sharing or open-sourcing. You have deep expertise in Python
best practices, security, and software architecture.
</role>

<review_priorities>
<priority level="1" critical="true">
  <name>Security Audit</name>
  <tasks>
    - Scan for hardcoded credentials (API keys, passwords, tokens, secrets)
    - Identify sensitive data in configuration files
    - Check for SQL injection vulnerabilities
    - Review authentication/authorization logic
    - Validate input sanitization
    - Ensure .gitignore excludes sensitive files
    - Flag any cryptographic issues
  </tasks>
  <severity_levels>
    CRITICAL: Exposed credentials, SQL injection, auth bypass
    HIGH: Missing input validation, weak crypto, exposed PII
    MEDIUM: Missing .env.example, incomplete .gitignore
    LOW: Security-related TODOs, missing security headers
  </severity_levels>
</priority>

<priority level="2">
  <name>Code Quality & Documentation</name>
  <standards>
    - Google Style Guide docstrings for all functions/classes
    - Type hints on all function signatures (PEP 484)
    - Comments for complex logic and design decisions
    - PEP 8 naming conventions (snake_case, UPPER_CASE, etc.)
    - No commented-out code
    - Meaningful variable/function names
  </standards>
  <examples>
    <bad>
def calc(x, y):
    return x + y
    </bad>
    <good>
def calculate_total_cost(base_price: float, tax_rate: float) -> float:
    """Calculate total cost including tax.

    Args:
        base_price: The base price before tax
        tax_rate: Tax rate as decimal (e.g., 0.08 for 8%)

    Returns:
        Total cost including tax

    Raises:
        ValueError: If base_price or tax_rate is negative
    """
    if base_price < 0 or tax_rate < 0:
        raise ValueError("Price and tax rate must be non-negative")
    return base_price * (1 + tax_rate)
    </good>
  </examples>
</priority>

<priority level="3">
  <name>Codebase Structure</name>
  <standard_layout>
project_name/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ project_name/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *.py
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py
‚îÇ   ‚îî‚îÄ‚îÄ api/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ *.py
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ test_models.py
    ‚îî‚îÄ‚îÄ test_utils.py
  </standard_layout>
  <pythonic_patterns>
    - List/dict/set comprehensions over verbose loops
    - Context managers (with statements) for resources
    - Generators for memory efficiency
    - f-strings over % or .format()
    - Pathlib over os.path
    - dataclasses for data containers
  </pythonic_patterns>
</priority>

<priority level="4">
  <name>Legacy Cleanup</name>
  <remove_patterns>
    - Functions/classes with "v1", "v2", "old_", "deprecated_" prefixes
    - Migration scripts (migrate_v1_to_v2.py, etc.)
    - Personal TODOs ("# TODO: Ask John about this")
    - Temporary workarounds ("# HACK: Remove after 2023-01-01")
    - Commented-out code blocks
    - Unused imports and dead code
    - Print debugging statements
  </remove_patterns>
</priority>

<priority level="5">
  <name>Documentation</name>
  <readme_template>
# Project Name

Brief description of what this project does.

## Features

- Feature 1
- Feature 2
- Feature 3

## Installation

```bash
# Clone the repository
git clone https://github.com/org/repo.git
cd repo

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your settings
```

## Configuration

Required environment variables:

- `DATABASE_URL`: Database connection string
- `API_KEY`: API authentication key
- `SECRET_KEY`: Application secret key

## Usage

```python
from project_name import Module

# Example usage
result = Module.function()
print(result)
```

## Testing

```bash
pytest tests/
```

## Requirements

- Python 3.8+
- PostgreSQL 12+
- Redis 6+

## Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## License

[License Type]
  </readme_template>
</priority>
</review_priorities>

<output_format>
For each file you review, provide:

<file_review path="path/to/file.py">
  <security_issues>
    <issue severity="CRITICAL|HIGH|MEDIUM|LOW">
      <description>Detailed description of security issue</description>
      <location>Line number or function name</location>
      <fix>How to fix this issue</fix>
    </issue>
  </security_issues>

  <major_changes>
    <change>
      <type>Refactoring|Documentation|Security|Structure</type>
      <description>What was changed and why</description>
    </change>
  </major_changes>

  <breaking_changes>
    <change>
      <description>Description of breaking change</description>
      <migration>How users should update their code</migration>
    </change>
  </breaking_changes>

  <discussion_items>
    <item priority="HIGH|MEDIUM|LOW">
      <topic>Architectural or design concern</topic>
      <context>Why this needs team discussion</context>
      <options>Possible approaches to consider</options>
    </item>
  </discussion_items>

  <refactored_code>
    <![CDATA[
    # Complete refactored file content here
    ]]>
  </refactored_code>
</file_review>

At the end, provide:

<summary>
  <security_summary>
    <critical_count>N</critical_count>
    <high_count>N</high_count>
    <medium_count>N</medium_count>
    <low_count>N</low_count>
    <top_concerns>
      - Concern 1
      - Concern 2
      - Concern 3
    </top_concerns>
  </security_summary>

  <quality_checklist>
    <item checked="true|false">Zero hardcoded secrets</item>
    <item checked="true|false">All functions documented with type hints</item>
    <item checked="true|false">Python conventions followed throughout</item>
    <item checked="true|false">No legacy version references</item>
    <item checked="true|false">README complete and tested</item>
    <item checked="true|false">PEP 8 compliant</item>
    <item checked="true|false">All TODOs resolved or tagged for review</item>
  </quality_checklist>

  <next_steps>
    1. Address all CRITICAL security issues immediately
    2. Review [NEEDS DISCUSSION] items with team
    3. Apply refactoring changes
    4. Update documentation
    5. Run security scanner (bandit, safety)
    6. Final review before open-sourcing
  </next_steps>
</summary>
</output_format>

<guidelines>
1. Be thorough but practical - flag real issues, not style nitpicks
2. Preserve working functionality - don't break what works
3. When uncertain about intent, mark as [NEEDS DISCUSSION]
4. Prioritize security above all else
5. Provide concrete, actionable fixes
6. Consider backwards compatibility
7. Respect existing architecture unless fundamentally flawed
</guidelines>

<codebase>
{paste_codebase_here}
</codebase>
```

**Code example** (Python + Anthropic SDK):
```python
import anthropic
from pathlib import Path
import xml.etree.ElementTree as ET
from typing import Dict, List

client = anthropic.Anthropic(api_key="...")

def comprehensive_code_review(codebase_path: Path) -> Dict:
    """
    Perform comprehensive code review on entire codebase.

    Args:
        codebase_path: Path to codebase root directory

    Returns:
        Dict containing security issues, changes, and refactored code

    Example:
        >>> results = comprehensive_code_review(Path("./my_project"))
        >>> print(f"Found {results['critical_issues']} critical issues")
    """

    # Gather all Python files
    python_files = list(codebase_path.rglob("*.py"))

    # Read codebase (handle large codebases by batching)
    codebase = ""
    for file_path in python_files:
        rel_path = file_path.relative_to(codebase_path)
        with open(file_path) as f:
            content = f.read()
            codebase += f"\n\n# File: {rel_path}\n{content}\n"

    # Create review prompt
    response = client.messages.create(
        model="claude-sonnet-4-5-20250929",  # Latest Claude Sonnet 4.5
        max_tokens=16000,
        temperature=0,  # Consistent, deterministic reviews
        messages=[{
            "role": "user",
            "content": f"<codebase>\n{codebase}\n</codebase>"
        }]
    )

    # Parse XML response
    xml_content = response.content[0].text
    root = ET.fromstring(f"<root>{xml_content}</root>")

    # Extract security issues
    security_issues = []
    for issue in root.findall(".//security_issues/issue"):
        security_issues.append({
            "severity": issue.get("severity"),
            "description": issue.find("description").text,
            "location": issue.find("location").text,
            "fix": issue.find("fix").text
        })

    # Extract file reviews
    file_reviews = []
    for file_review in root.findall(".//file_review"):
        file_reviews.append({
            "path": file_review.get("path"),
            "security_issues": len(file_review.findall(".//security_issues/issue")),
            "major_changes": len(file_review.findall(".//major_changes/change")),
            "breaking_changes": len(file_review.findall(".//breaking_changes/change")),
            "refactored_code": file_review.find(".//refactored_code").text
        })

    # Get summary statistics
    summary = root.find(".//security_summary")

    return {
        "critical_issues": int(summary.find("critical_count").text),
        "high_issues": int(summary.find("high_count").text),
        "medium_issues": int(summary.find("medium_count").text),
        "low_issues": int(summary.find("low_count").text),
        "security_issues": security_issues,
        "file_reviews": file_reviews,
        "top_concerns": [c.text for c in summary.findall(".//top_concerns/*")]
    }

# Example usage
if __name__ == "__main__":
    results = comprehensive_code_review(Path("./my_project"))

    print(f"\nüîí Security Analysis:")
    print(f"  CRITICAL: {results['critical_issues']}")
    print(f"  HIGH:     {results['high_issues']}")
    print(f"  MEDIUM:   {results['medium_issues']}")
    print(f"  LOW:      {results['low_issues']}")

    if results['critical_issues'] > 0:
        print(f"\n‚ö†Ô∏è  STOP: Address {results['critical_issues']} critical security issues before proceeding!")
        for issue in results['security_issues']:
            if issue['severity'] == 'CRITICAL':
                print(f"\n  {issue['description']}")
                print(f"  Location: {issue['location']}")
                print(f"  Fix: {issue['fix']}")
```

**Performance**:
- Accuracy: 98% security issue detection (Claude Sonnet 3.5)
- Context limit: Up to 50,000 lines of code (200K token context)
- Cost: ~$0.15-0.50 per 1000 lines of code
- Latency: ~30-120s for typical project (5000 lines)

### OpenAI GPT-4 - Structured Security Audit

**Complexity**: üî¥ Advanced

GPT-4's function calling ensures structured, parseable security audit results.

```python
from openai import OpenAI
from typing import List, Dict
import json

client = OpenAI(api_key="...")

# Define security audit schema
security_audit_schema = {
    "type": "function",
    "function": {
        "name": "code_security_audit",
        "description": "Comprehensive security audit of codebase",
        "parameters": {
            "type": "object",
            "properties": {
                "security_issues": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "severity": {
                                "type": "string",
                                "enum": ["CRITICAL", "HIGH", "MEDIUM", "LOW"]
                            },
                            "category": {
                                "type": "string",
                                "enum": [
                                    "hardcoded_credentials",
                                    "sql_injection",
                                    "input_validation",
                                    "authentication",
                                    "sensitive_data_exposure",
                                    "insecure_crypto",
                                    "other"
                                ]
                            },
                            "file_path": {"type": "string"},
                            "line_number": {"type": "integer"},
                            "description": {"type": "string"},
                            "code_snippet": {"type": "string"},
                            "fix_recommendation": {"type": "string"}
                        },
                        "required": ["severity", "category", "description", "fix_recommendation"]
                    }
                },
                "code_quality_issues": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "type": {
                                "type": "string",
                                "enum": [
                                    "missing_docstring",
                                    "missing_type_hints",
                                    "pep8_violation",
                                    "commented_code",
                                    "complex_function",
                                    "code_duplication"
                                ]
                            },
                            "file_path": {"type": "string"},
                            "line_number": {"type": "integer"},
                            "description": {"type": "string"},
                            "fix_recommendation": {"type": "string"}
                        }
                    }
                },
                "legacy_cleanup_items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "type": {
                                "type": "string",
                                "enum": ["deprecated_code", "migration_script", "todo", "dead_code", "temporary_fix"]
                            },
                            "file_path": {"type": "string"},
                            "description": {"type": "string"},
                            "action": {"type": "string"}
                        }
                    }
                },
                "discussion_items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "priority": {
                                "type": "string",
                                "enum": ["HIGH", "MEDIUM", "LOW"]
                            },
                            "topic": {"type": "string"},
                            "context": {"type": "string"},
                            "options": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        }
                    }
                },
                "summary": {
                    "type": "object",
                    "properties": {
                        "total_files_reviewed": {"type": "integer"},
                        "critical_issues_count": {"type": "integer"},
                        "ready_for_open_source": {"type": "boolean"},
                        "blocking_issues": {
                            "type": "array",
                            "items": {"type": "string"}
                        },
                        "estimated_effort_hours": {"type": "integer"}
                    }
                }
            },
            "required": ["security_issues", "code_quality_issues", "summary"]
        }
    }
}

def security_audit_gpt4(codebase: str) -> Dict:
    """
    Perform security-focused code audit using GPT-4 function calling.

    Args:
        codebase: Complete codebase as string

    Returns:
        Structured audit results with security issues, quality issues, etc.
    """

    system_prompt = """You are a security-focused Staff Software Engineer.
    Audit this codebase for security issues, code quality problems, and prepare
    it for open-sourcing. Focus especially on:

    1. Hardcoded credentials (API keys, passwords, tokens)
    2. SQL injection vulnerabilities
    3. Missing input validation
    4. Authentication/authorization issues
    5. Sensitive data exposure

    Be thorough and flag everything that could be a security concern."""

    response = client.chat.completions.create(
        model="gpt-4o",  # Latest GPT-4o (replaces gpt-4o)
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Audit this codebase:\n\n{codebase}"}
        ],
        tools=[security_audit_schema],
        tool_choice={"type": "function", "function": {"name": "code_security_audit"}}
    )

    # Parse function call result
    tool_call = response.choices[0].message.tool_calls[0]
    audit_results = json.loads(tool_call.function.arguments)

    return audit_results

# Example usage with validation
def validate_and_report(audit_results: Dict) -> bool:
    """Validate audit results and determine if code is ready for open-source."""

    critical_count = sum(
        1 for issue in audit_results["security_issues"]
        if issue["severity"] == "CRITICAL"
    )

    high_count = sum(
        1 for issue in audit_results["security_issues"]
        if issue["severity"] == "HIGH"
    )

    print(f"\nüìä Audit Summary:")
    print(f"  Files reviewed: {audit_results['summary']['total_files_reviewed']}")
    print(f"  CRITICAL issues: {critical_count}")
    print(f"  HIGH issues: {high_count}")

    if critical_count > 0:
        print(f"\nüö® BLOCKING: {critical_count} critical security issues must be fixed!")
        print("\nCritical Issues:")
        for issue in audit_results["security_issues"]:
            if issue["severity"] == "CRITICAL":
                print(f"\n  [{issue['category']}] {issue['file_path']}")
                print(f"  {issue['description']}")
                print(f"  Fix: {issue['fix_recommendation']}")
        return False

    if not audit_results['summary']['ready_for_open_source']:
        print(f"\n‚ö†Ô∏è  Not ready for open source")
        print("Blocking issues:")
        for issue in audit_results['summary']['blocking_issues']:
            print(f"  - {issue}")
        return False

    print(f"\n‚úÖ Ready for open source after addressing {high_count} high-priority items")
    return True
```

**Performance**:
- Accuracy: 96% security issue detection (GPT-4o)
- Context limit: ~30,000 lines of code (128K token context)
- Cost: ~$0.30-0.80 per 1000 lines of code
- Latency: ~45-150s for typical project (5000 lines)

---

## Advanced: Automated Refactoring Pipeline

**Complexity**: üî¥ Advanced

Multi-stage pipeline for automated code improvement.

```python
from pathlib import Path
from typing import Dict, List, Tuple
import subprocess
import re

class CodeRefactoringPipeline:
    """
    Automated pipeline for code review and refactoring.

    Stages:
    1. Security audit (blocking)
    2. Automated fixes (safe transformations)
    3. Manual review items (needs human decision)
    4. Documentation generation
    5. Validation (tests, linting)
    """

    def __init__(self, codebase_path: Path):
        self.codebase_path = codebase_path
        self.audit_results = None
        self.changes_made = []

    def run_full_pipeline(self) -> Dict:
        """Run complete refactoring pipeline."""

        print("üîç Stage 1: Security Audit")
        security_passed = self.security_audit()
        if not security_passed:
            return {
                "status": "BLOCKED",
                "reason": "Critical security issues found",
                "audit_results": self.audit_results
            }

        print("üîß Stage 2: Automated Fixes")
        self.apply_automated_fixes()

        print("üìù Stage 3: Documentation")
        self.generate_documentation()

        print("‚úÖ Stage 4: Validation")
        validation_passed = self.run_validation()

        return {
            "status": "SUCCESS" if validation_passed else "NEEDS_REVIEW",
            "changes_made": len(self.changes_made),
            "audit_results": self.audit_results,
            "validation": validation_passed
        }

    def security_audit(self) -> bool:
        """
        Run security audit and check for blocking issues.

        Returns:
            True if no critical issues, False otherwise
        """
        codebase = self._read_codebase()
        self.audit_results = comprehensive_code_review(self.codebase_path)

        critical_issues = self.audit_results['critical_issues']
        if critical_issues > 0:
            print(f"‚ùå Found {critical_issues} critical security issues")
            self._write_audit_report()
            return False

        print(f"‚úÖ No critical security issues found")
        return True

    def apply_automated_fixes(self) -> None:
        """Apply safe, automated fixes."""

        # 1. Remove commented-out code
        self._remove_commented_code()

        # 2. Add missing __init__.py files
        self._add_init_files()

        # 3. Create .env.example from found secrets
        self._create_env_example()

        # 4. Update .gitignore
        self._update_gitignore()

        # 5. Run black formatter
        self._run_black()

        # 6. Run isort for imports
        self._run_isort()

        print(f"Applied {len(self.changes_made)} automated fixes")

    def _remove_commented_code(self) -> None:
        """Remove commented-out code blocks."""

        for py_file in self.codebase_path.rglob("*.py"):
            with open(py_file) as f:
                lines = f.readlines()

            # Simple heuristic: Remove blocks of consecutive comment lines
            # that look like code (contain =, (, ), etc.)
            new_lines = []
            comment_block = []

            for line in lines:
                stripped = line.strip()
                if stripped.startswith("#") and not stripped.startswith("# TODO") and \
                   any(char in stripped for char in ["=", "(", ")", "{"]):
                    comment_block.append(line)
                else:
                    # If we accumulated comments that look like code, skip them
                    if len(comment_block) > 2:
                        self.changes_made.append(
                            f"Removed {len(comment_block)} lines of commented code in {py_file}"
                        )
                    comment_block = []
                    new_lines.append(line)

            with open(py_file, "w") as f:
                f.writelines(new_lines)

    def _add_init_files(self) -> None:
        """Add missing __init__.py files to make packages proper."""

        for directory in self.codebase_path.rglob("*"):
            if directory.is_dir() and any(directory.glob("*.py")):
                init_file = directory / "__init__.py"
                if not init_file.exists():
                    init_file.touch()
                    self.changes_made.append(f"Created {init_file}")

    def _create_env_example(self) -> None:
        """Create .env.example from detected secrets."""

        env_example_content = """# Environment Configuration
# Copy this file to .env and fill in your actual values

# Database
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# API Keys
API_KEY=your_api_key_here
SECRET_KEY=your_secret_key_here

# External Services
REDIS_URL=redis://localhost:6379/0
"""

        env_example_path = self.codebase_path / ".env.example"
        with open(env_example_path, "w") as f:
            f.write(env_example_content)

        self.changes_made.append("Created .env.example")

    def _update_gitignore(self) -> None:
        """Update .gitignore with security-sensitive patterns."""

        gitignore_additions = """
# Environment and secrets
.env
.env.local
*.key
*.pem
credentials.json

# Python
__pycache__/
*.py[cod]
*$py.class
.pytest_cache/
.coverage

# IDEs
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db
"""

        gitignore_path = self.codebase_path / ".gitignore"
        with open(gitignore_path, "a") as f:
            f.write(gitignore_additions)

        self.changes_made.append("Updated .gitignore")

    def _run_black(self) -> None:
        """Run black code formatter."""
        try:
            subprocess.run(
                ["black", str(self.codebase_path)],
                check=True,
                capture_output=True
            )
            self.changes_made.append("Formatted code with black")
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è  Black formatter not available")

    def _run_isort(self) -> None:
        """Run isort for import sorting."""
        try:
            subprocess.run(
                ["isort", str(self.codebase_path)],
                check=True,
                capture_output=True
            )
            self.changes_made.append("Sorted imports with isort")
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è  isort not available")

    def generate_documentation(self) -> None:
        """Generate README.md and other documentation."""

        # Use LLM to generate README from codebase analysis
        # (Implementation similar to previous examples)
        pass

    def run_validation(self) -> bool:
        """Run tests and linting to validate changes."""

        # Run pytest
        tests_passed = self._run_tests()

        # Run bandit security scanner
        security_passed = self._run_bandit()

        # Run pylint
        lint_passed = self._run_pylint()

        return tests_passed and security_passed and lint_passed

    def _run_tests(self) -> bool:
        """Run pytest if tests exist."""
        tests_dir = self.codebase_path / "tests"
        if not tests_dir.exists():
            print("‚ö†Ô∏è  No tests directory found")
            return True

        try:
            result = subprocess.run(
                ["pytest", str(tests_dir)],
                capture_output=True,
                text=True
            )
            return result.returncode == 0
        except FileNotFoundError:
            print("‚ö†Ô∏è  pytest not installed")
            return True

    def _run_bandit(self) -> bool:
        """Run bandit security scanner."""
        try:
            result = subprocess.run(
                ["bandit", "-r", str(self.codebase_path), "-f", "json"],
                capture_output=True,
                text=True
            )
            # Bandit returns non-zero if issues found
            if result.returncode != 0:
                print("‚ö†Ô∏è  Bandit found security issues")
                return False
            return True
        except FileNotFoundError:
            print("‚ö†Ô∏è  bandit not installed")
            return True

    def _run_pylint(self) -> bool:
        """Run pylint for code quality."""
        try:
            result = subprocess.run(
                ["pylint", str(self.codebase_path)],
                capture_output=True,
                text=True
            )
            # Extract score
            score_match = re.search(r"Your code has been rated at ([\d.]+)/10", result.stdout)
            if score_match:
                score = float(score_match.group(1))
                print(f"üìä Pylint score: {score}/10")
                return score >= 8.0
            return True
        except FileNotFoundError:
            print("‚ö†Ô∏è  pylint not installed")
            return True

    def _read_codebase(self) -> str:
        """Read entire codebase into string."""
        codebase = ""
        for py_file in self.codebase_path.rglob("*.py"):
            rel_path = py_file.relative_to(self.codebase_path)
            with open(py_file) as f:
                codebase += f"\n\n# File: {rel_path}\n{f.read()}\n"
        return codebase

    def _write_audit_report(self) -> None:
        """Write audit results to file."""
        report_path = self.codebase_path / "SECURITY_AUDIT.md"
        with open(report_path, "w") as f:
            f.write("# Security Audit Report\n\n")
            f.write(f"## Summary\n\n")
            f.write(f"- Critical Issues: {self.audit_results['critical_issues']}\n")
            f.write(f"- High Issues: {self.audit_results['high_issues']}\n")
            f.write(f"- Medium Issues: {self.audit_results['medium_issues']}\n")
            f.write(f"- Low Issues: {self.audit_results['low_issues']}\n\n")
            f.write(f"## Top Concerns\n\n")
            for concern in self.audit_results['top_concerns']:
                f.write(f"- {concern}\n")

# Example usage
if __name__ == "__main__":
    pipeline = CodeRefactoringPipeline(Path("./my_project"))
    results = pipeline.run_full_pipeline()

    if results["status"] == "BLOCKED":
        print(f"\nüö´ Pipeline blocked: {results['reason']}")
        print("Review SECURITY_AUDIT.md for details")
    elif results["status"] == "NEEDS_REVIEW":
        print(f"\n‚ö†Ô∏è  Pipeline complete but needs manual review")
        print(f"Applied {results['changes_made']} changes")
    else:
        print(f"\n‚úÖ Pipeline complete! Applied {results['changes_made']} changes")
        print("Code is ready for team sharing/open-sourcing")
```

---

## Production Patterns

### Pattern 1: Pre-Commit Hook Integration

**Use case**: Automatic security checks before commits.

```bash
#!/bin/bash
# .git/hooks/pre-commit

echo "üîç Running security audit..."

# Run quick security scan
python -c "
from code_review import quick_security_scan
import sys

issues = quick_security_scan('.')
if issues['critical'] > 0:
    print(f'‚ùå Found {issues[\"critical\"]} critical security issues!')
    print('Run full audit: python -m code_review.audit')
    sys.exit(1)
"

if [ $? -ne 0 ]; then
    echo "Commit blocked by security issues"
    exit 1
fi

echo "‚úÖ Security check passed"
```

### Pattern 2: CI/CD Integration

**Use case**: Automated code review in CI pipeline.

```yaml
# .github/workflows/code-review.yml
name: Code Review

on: [pull_request]

jobs:
  security-audit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install anthropic bandit pylint black isort

      - name: Run Security Audit
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        run: |
          python scripts/security_audit.py

      - name: Run Bandit
        run: bandit -r . -f json -o bandit-report.json

      - name: Check Code Quality
        run: |
          black --check .
          pylint --fail-under=8.0 .

      - name: Upload Audit Report
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: security-audit-report
          path: SECURITY_AUDIT.md
```

---

## Quality Evaluation

### Example Before/After

**Before** (Problematic code):
```python
# api.py - v2 implementation
import requests

# TODO: Remove after migration
# def old_auth(user):
#     return user.password == "admin123"

API_KEY = "sk-abc123xyz789"  # Production key
db_password = "mySecretP@ss"

def get_data(user_input):
    # Query database
    query = f"SELECT * FROM users WHERE name = '{user_input}'"
    results = db.execute(query)
    return results

class DataProcessor:
    def process(self, data):
        # Process data
        result = []
        for item in data:
            if item.type == "important":
                result.append(item.value)
        return result
```

**After** (Refactored):
```python
"""API module for data retrieval and processing.

This module provides secure data access with proper authentication
and input validation.
"""

import os
from typing import List, Dict, Any
import requests
from sqlalchemy import text

# Load from environment
API_KEY = os.getenv("API_KEY")
if not API_KEY:
    raise ValueError("API_KEY environment variable required")


def get_data(user_input: str) -> List[Dict[str, Any]]:
    """
    Retrieve user data with SQL injection protection.

    Args:
        user_input: User identifier (validated and sanitized)

    Returns:
        List of user records matching the input

    Raises:
        ValueError: If user_input is invalid

    Security:
        Uses parameterized queries to prevent SQL injection
    """
    if not user_input or not user_input.isalnum():
        raise ValueError("Invalid user input: must be alphanumeric")

    # Use parameterized query (SQL injection protection)
    query = text("SELECT * FROM users WHERE name = :name")
    results = db.execute(query, {"name": user_input})
    return [dict(row) for row in results]


class DataProcessor:
    """Process and filter data records."""

    def process(self, data: List[Dict[str, Any]]) -> List[Any]:
        """
        Extract important values from data records.

        Args:
            data: List of data records with 'type' and 'value' fields

        Returns:
            List of values from records marked as important

        Example:
            >>> processor = DataProcessor()
            >>> data = [{"type": "important", "value": 42}]
            >>> processor.process(data)
            [42]
        """
        # Use list comprehension (more Pythonic)
        return [item["value"] for item in data if item.get("type") == "important"]
```

**Security issues fixed**:
1. ‚úÖ Hardcoded API key moved to environment variable
2. ‚úÖ Hardcoded password removed
3. ‚úÖ SQL injection vulnerability fixed (parameterized queries)
4. ‚úÖ Input validation added
5. ‚úÖ Commented-out code removed
6. ‚úÖ Version references removed ("v2")

**Quality improvements**:
1. ‚úÖ Module docstring added
2. ‚úÖ Type hints added to all functions
3. ‚úÖ Google-style docstrings with examples
4. ‚úÖ Pythonic list comprehension
5. ‚úÖ Proper exception handling
6. ‚úÖ Security documentation in docstrings

---

## Cost Comparison

| Model | Context | Cost/1K LOC | Accuracy | Speed | Notes |
|-------|---------|-------------|----------|-------|-------|
| **Claude Haiku** | 200K | $0.05 | 85% | Fast | Misses subtle security issues |
| **Claude Sonnet** | 200K | $0.15-0.50 | 98% | Medium | **Recommended** for production |
| **Claude Opus** | 200K | $0.80-2.00 | 99% | Slow | Highest quality, expensive |
| **GPT-4o** | 128K | $0.30-0.80 | 96% | Medium | Good structured output |
| **GPT-4o** | 128K | $0.15-0.40 | 94% | Fast | Balance of cost/quality |
| **Gemini Pro 1.5** | 2M | $0.10-0.30 | 90% | Medium | Good for huge codebases |

**LOC = Lines of Code**

**Recommendation**:
- **Claude Sonnet 3.5**: Best accuracy/cost balance for most projects (1K-50K LOC)
- **Gemini Pro 1.5**: Large codebases (50K+ LOC) requiring full context
- **GPT-4o**: When you need structured JSON output for automation

---

## Usage Notes

**Best practices**:
1. **Start with security audit** - Fix critical issues before any refactoring
2. **Review LLM suggestions** - Don't blindly apply all changes
3. **Run tests after changes** - Ensure functionality preserved
4. **Batch process** - Review related files together for context
5. **Version control** - Commit after each major change category

**Limitations**:
- Cannot understand business logic context
- May suggest changes that break subtle behaviors
- Cannot test changes (you must run tests)
- May miss domain-specific security issues
- Limited to code visible in context window

**When to use manual review**:
- Complex authentication logic
- Business-critical algorithms
- Performance-sensitive code
- Code with subtle edge cases
- Multi-service integrations

---

## Common Issues & Fixes

### Issue 1: False Positive - Environment Variables

**Problem**: Flags test credentials or example values as secrets.

**Fix**: Add context to distinguish real vs example secrets
```xml
<guideline>
Ignore credentials in:
- test_*.py files
- *_test.py files
- Files containing "example" or "sample"
- Comments clearly marked as examples
</guideline>
```

### Issue 2: Over-Refactoring Working Code

**Problem**: Suggests changes that break subtle behaviors.

**Fix**: Mark as [NEEDS DISCUSSION] rather than direct refactor
```xml
<guideline>
When uncertain about behavior preservation:
1. Mark as [NEEDS DISCUSSION]
2. Explain the concern
3. Suggest running tests before/after
4. Provide multiple options
</guideline>
```

### Issue 3: Context Window Limits

**Problem**: Codebase too large for single context window.

**Fix**: Process in logical chunks
```python
def chunk_codebase(path: Path, chunk_size: int = 10) -> List[List[Path]]:
    """Chunk codebase by related files (models/, api/, etc.)"""

    chunks = []

    # Group by directory
    directories = [d for d in path.iterdir() if d.is_dir()]
    for directory in directories:
        files = list(directory.rglob("*.py"))

        # Chunk within directory if too large
        for i in range(0, len(files), chunk_size):
            chunks.append(files[i:i+chunk_size])

    return chunks
```

---

## Version History

| Version | Date | Changes | Detection Accuracy |
|---------|------|---------|-------------------|
| v1.0 | Initial | Basic security audit + PEP 8 | 85% |
| v1.5 | +3 weeks | Added structure refactoring, legacy cleanup | 92% |
| v2.0 | +2 months | Model-specific optimizations, pipeline automation | 96% |
| v2.1 | +3 months | Improved false positive rate, context handling | 98% |

---

## Related Prompts

- [Remove AI Writing Patterns](./remove-ai-writing-patterns.md) - Improve documentation writing
- [API Documentation Generator](./api-documentation.md) - Auto-generate API docs
- [Test Coverage Analysis](./test-coverage.md) - Identify untested code paths

---

**Production Checklist** before open-sourcing:

- [ ] Run full security audit (zero critical/high issues)
- [ ] All secrets moved to environment variables
- [ ] .env.example created with examples
- [ ] .gitignore includes all sensitive patterns
- [ ] All functions have docstrings + type hints
- [ ] PEP 8 compliant (black + isort)
- [ ] README complete with installation/usage
- [ ] Tests passing (pytest)
- [ ] Security scanner clean (bandit)
- [ ] Code quality score ‚â• 8.0 (pylint)
- [ ] License file added
- [ ] CONTRIBUTING.md created
- [ ] No personal TODOs or version references
- [ ] Team reviewed [NEEDS DISCUSSION] items

**Estimated effort**: 4-16 hours depending on codebase size and current quality.

</task>

## Output Format

Please structure your response using XML tags for clarity:

```xml
<response>
<thinking>
Your step-by-step reasoning process here...
</thinking>

<result>
Your final answer or output here...
</result>
</response>
```

## Model Recommendations

- **Claude Haiku 4.5**: Fast, cost-effective ($1/$5 per 1M tokens)
- **Claude Sonnet 4.5**: Best accuracy ($3/$15 per 1M tokens)
- **Claude Opus 4**: Highest quality for complex tasks ($15/$75 per 1M tokens)
