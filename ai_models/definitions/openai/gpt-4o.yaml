# GPT-4o Model Definition
# Last verified: 2025-11-01
# Source: https://platform.openai.com/docs/models

schema_version: "1.0.0"
model_id: "gpt-4o"
provider: "openai"
name: "GPT-4o"
api_identifier: "gpt-4o-2024-08-06"

metadata:
  context_window_input: 128000
  context_window_output: 16384
  knowledge_cutoff: "October 2023"
  release_date: "2024-08-06"
  last_verified: "2025-11-01"
  docs_url: "https://platform.openai.com/docs/models/gpt-4o"

capabilities:
  - text_input
  - text_output
  - function_calling
  - vision
  - voice
  - realtime_api
  - large_context
  - streaming
  - json_mode

pricing:
  input_per_1m: 2.50
  output_per_1m: 10.00

optimization:
  recommended_for:
    - "Multimodal specialist (voice, vision, text)"
    - "GPT-4o Realtime API for live audio"
    - "Voice-based applications"
    - "Vision-heavy applications"
    - "Low-latency multimodal tasks"
    - "Function calling workflows"
    - "Structured JSON output"
  best_practices:
    - "Primary use case: multimodal applications requiring voice or vision"
    - "Use GPT-4o Realtime API for live audio applications"
    - "Leverage vision capability for image understanding"
    - "Use JSON mode for structured outputs"
    - "Function calling for tool integration"
    - "Set max_tokens to control output costs"
    - "Stream responses for better UX"
    - "For general tasks, consider GPT-5 instead"
  cost_tier: "mid-tier"
  speed_tier: "balanced"

notes: "Multimodal specialist for voice and vision. Still available alongside GPT-5. 128k context window. Best for voice/vision-heavy applications."
