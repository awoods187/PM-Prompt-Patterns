# AI PM Toolkit

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code of Conduct](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CODE_OF_CONDUCT.md)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

**Production-grade prompts and frameworks for AI-native product management**

A comprehensive collection of battle-tested prompt patterns, model optimization strategies, and production frameworks for product managers building with LLMs. Built on real-world experience deploying AI systems processing 5K+ heterogeneous data points weekly with 95% accuracy and monitoring $100M+ ARR.

## Why This Exists

Most prompt libraries focus on toy examples. This toolkit provides **production-grade patterns** with:

- âœ… **Real metrics** from production systems (95% accuracy, $0.001/signal cost)
- âœ… **Cost optimization** strategies (99.7% cost reduction examples)
- âœ… **Multi-model expertise** across Claude, GPT, and Gemini families
- âœ… **Token-level optimization** (prompt caching, hybrid classification, model cascading)
- âœ… **Quality frameworks** (evaluation methodology, test sets, continuous monitoring)
- âœ… **Production model management** (YAML-based registry, pricing service, capability validation)

## Quick Start

### Installation

For Python integration:

```bash
# Clone the repository
git clone https://github.com/awoods187/PM-Prompt-Patterns.git
cd PM-Prompt-Patterns

# Install as package (optional)
pip install -e .

# Verify installation
python -c "from ai_models import get_model; print(get_model('claude-sonnet-4-5').name)"
```

### By Experience Level

**ğŸŸ¢ Getting Started** â†’ Read [PROMPT_DESIGN_PRINCIPLES.md](./PROMPT_DESIGN_PRINCIPLES.md) for core patterns

**ğŸŸ¡ Optimizing Prompts** â†’ Explore [MODEL_OPTIMIZATION_GUIDE.md](./MODEL_OPTIMIZATION_GUIDE.md) for provider-specific techniques

**ğŸ”´ Production Systems** â†’ Study [examples/epic-categorization](./examples/epic-categorization/) for end-to-end architecture

### By Use Case

| Use Case            | Prompt       | Complexity | Models |
|---------------------|--------------|------------|--------|
| Analytics & Metrics | [analytics/](./prompts/analytics/) | ğŸŸ¢-ğŸ”´ All levels | All models |
| Epic Categorization | [epic-categorization](./examples/epic-categorization/) | ğŸŸ¡ Intermediate | Claude Sonnet, GPT-4 |
| Product Strategy    | [product-strategy/](./prompts/product-strategy/) | ğŸŸ¡-ğŸ”´ Intermediate-Advanced | Claude Opus, Sonnet |

## Model Selection Guide

Understanding **when** to use each model is as important as **how** to prompt them.

### Quick Reference

| Model                    | Cost (Input/Output per 1M tokens) | Context | Best For | Production Use |
|--------------------------|---------|---------|----------|----------------|
| **Claude Haiku 4.5** | $1.00 / $5.00 | 200K | High-volume classification (1000s/day) | 70% of our workload |
| **Claude Sonnet 4.5** | $3.00 / $15.00 | 200K | Production workhorse, complex analysis | 25% of our workload |
| **Claude Opus 4.1** | $15.00 / $75.00 | 200K | High-stakes decisions, creative work | 5% of our workload |
| **GPT-4o** | $2.50 / $10.00 | 128K | Multimodal, function calling, structured extraction | Specific use cases |
| **GPT-4o mini** | $0.15 / $0.60 | 128K | Cost-efficient alternative to 3.5 Turbo | Budget tasks |
| **Gemini 2.5 Pro** | $1.25 / $5.00 | 2M | Massive context analysis (entire codebases) | Large document analysis |
| **Gemini 2.5 Flash** | $0.075 / $0.30 | 1M | Speed-critical, high-volume applications | Real-time features |
| **Gemini 2.5 Flash-Lite** | Coming Soon | 1M | Ultra cost-efficient processing | Maximum throughput |

*Pricing verified October 2025. Managed via `ai_models` system - see [MODEL_OPTIMIZATION_GUIDE.md](./MODEL_OPTIMIZATION_GUIDE.md).*

### Model Cascading Pattern

Don't use one model for everything. **Route intelligently**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input Signal   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Keyword Filter  â”‚â”€â”€â”€â”€ 70% caught â”€â”€â”€â”€ FREE
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 30% through
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Haiku    â”‚â”€â”€â”€â”€ 25% caught â”€â”€â”€â”€ $0.0003/signal
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 5% through
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Sonnet   â”‚â”€â”€â”€â”€ 4.5% caught â”€â”€â”€ $0.002/signal
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 0.5% through
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Claude Opus     â”‚â”€â”€â”€â”€ 0.5% caught â”€â”€â”€ $0.015/signal
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Average cost per signal: $0.001
Naive Opus-only approach: $0.015 (15x more expensive)
```

See [Cost Optimization Guide](./docs/cost-optimization.md) for implementation details.

## AI Model Management System

This repository includes a production-grade model management system (`ai_models/`) with:

### Key Features

**âœ… YAML-based Model Definitions**: Version-controlled model specifications
```python
from ai_models import get_model

model = get_model("claude-sonnet-4-5")
print(f"Context: {model.metadata.context_window_input:,} tokens")
print(f"Cost: ${model.pricing.input_per_1m}/M input")
# Context: 200,000 tokens
# Cost: $3.0/M input
```

**âœ… Runtime Capability Validation**: Check model features before API calls
```python
from ai_models import has_vision, has_function_calling, has_prompt_caching

if has_vision("gpt-4o"):
    process_image()  # Safe - GPT-4o supports vision

if has_prompt_caching("claude-sonnet-4-5"):
    enable_caching()  # 90% cost savings on cached tokens
```

**âœ… Optimized Pricing Service**: LRU-cached cost calculations
```python
model = get_model("claude-haiku-4-5")
cost = model.calculate_cost(
    input_tokens=10_000,
    output_tokens=2_000,
    cached_input_tokens=5_000  # Automatic cache discount
)
# Cost: ~$0.013 (vs $0.025 without caching)
```

**âœ… Cost Tier Filtering**: Find budget-friendly models
```python
from ai_models import ModelRegistry

budget_models = ModelRegistry.filter_by_cost_tier("budget")
# Returns: Haiku, 4o-mini, Flash, Flash-Lite
```

**âœ… Comprehensive Testing**: 97 tests ensure pricing accuracy
- Fixed critical bug: Claude Haiku was 4x underpriced
- Prevents future pricing regressions
- Validates all model specs against official docs

See [MIGRATION_GUIDE.md](./MIGRATION_GUIDE.md) for migrating from old `models/registry.py` system.

## Featured Example: Signal Classification System

**Problem**: Manual classification of customer signals taking 8+ hours/week, inconsistent quality

**Solution**: Hybrid classification system combining keyword matching + LLM analysis

**Results**:
- âš¡ **15 minutes** end-to-end processing (vs 8 hours manual)
- ğŸ’° **$0.001 per signal** (99.7% cost reduction from naive approach)
- ğŸ¯ **95% accuracy** (vs 85% manual baseline)
- ğŸ“Š **2,000+ signals/week** processed automatically
- ğŸ—ï¸ **Production-tested** on systems monitoring $100M+ ARR

**Key Techniques**:
1. **Progressive enhancement**: Keyword â†’ Haiku â†’ Sonnet â†’ Opus (only when needed)
2. **Prompt caching**: 95% cache hit rate = 96% cost reduction on repeat patterns
3. **Batch processing**: Process signals in groups of 50-100 for 92% cost reduction
4. **Confidence-based escalation**: Automatic escalation when confidence < 0.85

[â†’ Read the full case study](./examples/epic-categorization/)

[â†’ Explore analytics prompts](./prompts/analytics/)

## Repository Structure

```
PM-Prompt-Patterns/
â”œâ”€â”€ prompts/                    # Production-ready prompts by category
â”‚   â”œâ”€â”€ analytics/              # âœ¨ NEW: Data analysis, metrics, reporting (MECE organized)
â”‚   â”‚   â”œâ”€â”€ monitoring/         # Real-time alerts, anomaly detection
â”‚   â”‚   â”œâ”€â”€ reporting/          # Periodic dashboards, business reviews
â”‚   â”‚   â””â”€â”€ investigation/      # Ad-hoc analysis, root cause deep dives
â”‚   â”œâ”€â”€ product-strategy/       # Roadmapping, prioritization
â”‚   â”œâ”€â”€ customer-research/      # User feedback analysis
â”‚   â”œâ”€â”€ roadmap-planning/       # Feature prioritization
â”‚   â”œâ”€â”€ stakeholder-communication/
â”‚   â””â”€â”€ technical-documentation/
â”œâ”€â”€ templates/                  # Reusable prompt patterns
â”‚   â”œâ”€â”€ meta-prompting.md       # Use LLMs to improve prompts
â”‚   â”œâ”€â”€ chain-of-thought.md     # Reasoning patterns
â”‚   â”œâ”€â”€ structured-output.md    # JSON/XML output design
â”‚   â””â”€â”€ few-shot-examples.md    # Example-based learning
â”œâ”€â”€ ai_models/                  # âœ¨ NEW: Unified model management
â”‚   â”œâ”€â”€ registry.py             # YAML-based model registry
â”‚   â”œâ”€â”€ pricing.py              # Pricing service with caching
â”‚   â”œâ”€â”€ capabilities.py         # Runtime capability validation
â”‚   â””â”€â”€ definitions/            # Model specifications (YAML)
â”‚       â”œâ”€â”€ anthropic/          # Claude Haiku, Sonnet, Opus 4.x
â”‚       â”œâ”€â”€ openai/             # GPT-4o, GPT-4o mini
â”‚       â””â”€â”€ google/             # Gemini 2.5 Pro, Flash, Flash-Lite
â”œâ”€â”€ pm_prompt_toolkit/          # Python package for production use
â”‚   â”œâ”€â”€ providers/              # LLM provider integrations
â”‚   â”œâ”€â”€ optimizers/             # Prompt optimization utilities
â”‚   â””â”€â”€ config/                 # Configuration management
â”œâ”€â”€ examples/                   # Complete production systems
â”‚   â””â”€â”€ epic-categorization/    # Real-world classification
â”œâ”€â”€ tests/                      # Comprehensive test suite (97 tests)
â”‚   â”œâ”€â”€ test_model_registry.py  # Registry validation
â”‚   â”œâ”€â”€ test_ai_models.py       # New system tests
â”‚   â””â”€â”€ test_pricing_consistency.py
â”œâ”€â”€ scripts/                    # Automation and testing
â”‚   â””â”€â”€ run_tests.sh            # Test runner with multiple modes
â””â”€â”€ docs/                       # Deep-dive guides
    â”œâ”€â”€ PROMPT_DESIGN_PRINCIPLES.md
    â”œâ”€â”€ MODEL_OPTIMIZATION_GUIDE.md
    â”œâ”€â”€ MIGRATION_GUIDE.md      # âœ¨ NEW: Old â†’ new system migration
    â””â”€â”€ REFACTOR_COMPLETE.md    # âœ¨ NEW: Refactor summary
```

## Complexity Levels

Prompts are tagged by complexity to help you level up progressively:

- ğŸŸ¢ **Basic**: Single-turn, straightforward prompts. Start here if you're new to prompt engineering.
- ğŸŸ¡ **Intermediate**: Multi-step reasoning, few-shot learning, structured outputs. Use when basic prompts plateau.
- ğŸ”´ **Advanced**: Chain-of-thought, meta-prompting, multi-model orchestration. Use for production systems at scale.

**When to level up complexity**:
- Basic â†’ Intermediate: When accuracy < 80% or inconsistent outputs
- Intermediate â†’ Advanced: When processing >100 signals/day or cost becomes significant
- Consider simpler approaches first: A well-crafted basic prompt often beats a poorly-designed advanced one

## Advanced Technique: Meta-Prompting

One of the most powerful patterns for prompt engineering is **using an LLM to improve your prompts without executing them**.

Example:
```
I need to refine this classification prompt, but DON'T execute it.

CURRENT PROMPT:
"""
Classify this customer signal as: feature_request, bug_report, or churn_risk
Signal: {input}
"""

REQUIREMENTS:
- Optimize for: accuracy and consistency
- Target model: Claude Sonnet
- Constraints: <1000 tokens

Please provide:
1. Analysis of weaknesses
2. Three refined versions
3. Expected accuracy improvement

DO NOT execute the prompt or provide example outputs.
```

This pattern helped us evolve from 82% â†’ 95% accuracy across 5 prompt iterations.

[â†’ See complete meta-prompting guide](./templates/meta-prompting.md)

## Core Principles

### 1. Model-Agnostic by Default

Base prompts work across all providers. Optimizations are **additive**:

**Base Prompt** (works everywhere):
```
Classify the customer signal into one of these categories:
- feature_request
- bug_report
- churn_risk
- expansion_signal
- general_feedback

Signal: {input}

Output only the category name.
```

**Claude Optimization** (add XML structure):
```xml
<classification_task>
  <categories>
    <category>feature_request</category>
    <category>bug_report</category>
    ...
  </categories>

  <signal>{input}</signal>

  <output_format>category_name_only</output_format>
</classification_task>
```

**GPT-4 Optimization** (add JSON mode):
```json
{
  "task": "classification",
  "categories": ["feature_request", "bug_report", ...],
  "signal": "{input}",
  "output": {
    "format": "json",
    "schema": {"category": "string"}
  }
}
```

### 2. Cost-Conscious

Every prompt includes cost analysis:
- Token count estimates
- Cost per execution
- Optimization opportunities
- Cheaper alternatives

### 3. Production-Tested

All prompts include:
- âœ… Real metrics from production use
- âœ… Failure modes and edge cases
- âœ… Quality evaluation methodology
- âœ… Version history showing iteration

### 4. Quantified Everything

Avoid vague claims. Every optimization shows:
- Before/after metrics
- Cost impact ($ and %)
- Accuracy delta
- Latency impact

## Learning Path

### Week 1: Fundamentals
1. Read [PROMPT_DESIGN_PRINCIPLES.md](./PROMPT_DESIGN_PRINCIPLES.md)
2. Try 3 basic prompts from [prompts/](./prompts/)
3. Learn [model selection framework](./MODEL_OPTIMIZATION_GUIDE.md#decision-framework)

### Week 2: Optimization
1. Study [cost optimization strategies](./docs/cost-optimization.md)
2. Implement [hybrid classification pattern](./examples/signal-classification/)
3. Practice [meta-prompting](./templates/meta-prompting.md)

### Week 3: Production
1. Build evaluation framework ([quality-evaluation.md](./docs/quality-evaluation.md))
2. Implement [model cascading](./MODEL_OPTIMIZATION_GUIDE.md#model-cascading)
3. Set up monitoring and regression testing

### Week 4: Advanced
1. Multi-model orchestration
2. Prompt caching strategies
3. Continuous improvement loops

## Contributing

This is a living repository. Contributions welcome for:

- âœ… New production-tested prompts with metrics
- âœ… Cost optimization strategies with before/after data
- âœ… Model comparison insights
- âœ… Quality evaluation methodologies

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

**Quality bar**: All contributions must include quantified results from real usage (no toy examples).

## License & Contributing

### License

This project is licensed under the **MIT License** - see [LICENSE](./LICENSE) for full details.

**TL;DR**: You can use this freely in commercial products, modify as needed, and don't need attribution (though it's appreciated).

**Key Points**:
- âœ… Commercial use allowed
- âœ… Modification allowed
- âœ… Distribution allowed
- âœ… Private use allowed
- âŒ No warranty provided
- âŒ No liability accepted

**Questions about licensing?**
- [LICENSE_FAQ.md](./LICENSE_FAQ.md) - Common questions answered
- [CONTENT_LICENSE.md](./CONTENT_LICENSE.md) - Prompt-specific clarifications
- [ATTRIBUTION.md](./ATTRIBUTION.md) - How to credit (optional)

### Contributing

We welcome contributions! This project thrives on:
- ğŸ“Š **Production-tested prompts** with real metrics
- ğŸ”§ **Code improvements** to ai_models system
- ğŸ“š **Documentation** enhancements
- ğŸ§ª **Test additions** and quality improvements

**How to contribute**:
1. Read [CONTRIBUTING.md](./CONTRIBUTING.md) - Quality standards and process
2. Check [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) - Community guidelines
3. Sign your commits with DCO (`git commit -s`)
4. Submit a PR using our [template](.github/pull_request_template.md)

**All contributors** are recognized in [ATTRIBUTION.md](./ATTRIBUTION.md).

## ğŸ“¦ Recent Changes: Analytics Category Reorganization

**What Changed** (October 2025):
- Consolidated `prompts/data-analysis/` and `prompts/metrics-reporting/` into new `prompts/analytics/` structure
- Organized by temporal pattern (monitoring, reporting, investigation) for MECE principles
- Added comprehensive navigation and category guidelines

**Migration**:
- Old `data-analysis/` â†’ Now `analytics/` (organized by use case)
- Old `metrics-reporting/` â†’ Now `analytics/reporting/`
- See [prompts/analytics/README.md](./prompts/analytics/) for navigation guide

**Why**: The new structure eliminates category overlap and makes it clearer which prompts to use for real-time monitoring vs. periodic reporting vs. ad-hoc investigation.

## Important Note

All examples are genericized for public sharing. No proprietary information, customer data, or company-specific details included. Metrics are approximate and rounded for privacy.

---

## Quick Links

**ğŸ“š Learning Path**: [PROMPT_DESIGN_PRINCIPLES.md](./PROMPT_DESIGN_PRINCIPLES.md) â†’ [MODEL_OPTIMIZATION_GUIDE.md](./MODEL_OPTIMIZATION_GUIDE.md) â†’ [examples/epic-categorization](./examples/epic-categorization/)

**ğŸ”§ Developer Guide**: [MIGRATION_GUIDE.md](./MIGRATION_GUIDE.md) for using the `ai_models` system

**âœ… Test Suite**: `./scripts/run_tests.sh` to run 97 tests validating all models

**ğŸ“Š Model Registry**: See `ai_models/definitions/` for current model specifications

Questions? Open an issue or contribute your own patterns.
