# PM Prompt Toolkit

Production-grade prompt patterns and AI model management for product teams building with Claude, GPT, and Gemini. Proven at scale: 5K+ signals/week, 95% accuracy, $0.001/signal cost.

## ğŸ“Š Project Status

![CI](https://github.com/awoods187/PM-Prompt-Patterns/workflows/CI/badge.svg)
![Python](https://img.shields.io/badge/python-3.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)
![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)

**Status:** Production-ready | **Last Updated:** 2025-10-27 | **Maintenance:** Active development

---

## ğŸ¯ Why This Exists

**Problem:** Most prompt libraries showcase toy examples. Production systems need battle-tested patterns with real metrics.

**Solution:** Enterprise-grade toolkit combining 200+ production prompts, multi-model orchestration, and cost optimization strategies.

**Key Benefits:**
- **Proven ROI:** 99.7% cost reduction through intelligent model cascading (Haiku â†’ Sonnet â†’ Opus)
- **Production Metrics:** 95% accuracy on 5K+ weekly signals, validated on $100M+ ARR systems
- **Multi-Model Expertise:** Optimized patterns for Claude 4.x, GPT-4o, Gemini 2.5 families
- **Zero to Production:** Complete examples with evaluation methodology, not just prompts
- **Developer Tools:** Python package with YAML-based model registry, pricing service, capability validation

---

## ğŸš€ Quick Start

Get operational in under 5 minutes:

```bash
# Clone and install
git clone https://github.com/awoods187/PM-Prompt-Patterns.git
cd PM-Prompt-Patterns
pip install -e .

# Verify installation
python -c "from ai_models import get_model; print(get_model('claude-sonnet-4-5').name)"
```

**Next Steps by Role:**
- **New to prompts?** â†’ [Design Principles](./PROMPT_DESIGN_PRINCIPLES.md)
- **Optimizing costs?** â†’ [Model Selection Guide](#-model-selection-guide)
- **Building production systems?** â†’ [Epic Categorization Example](./examples/epic-categorization/)

---

## ğŸ“‹ Prerequisites

| Tool | Version | Verification | Purpose |
|------|---------|--------------|---------|
| Python | 3.9+ | `python --version` | Core runtime |
| pip | Latest | `pip --version` | Package management |
| API Keys | - | Set in `.env` | Claude/GPT/Gemini access |

<details>
<summary>API Key Setup (Optional)</summary>

Create `.env` file:
```bash
ANTHROPIC_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here
GOOGLE_API_KEY=your_key_here
```

Keys only required for live API testing. Browse prompts without keys.
</details>

---

## ğŸ“– Usage

### Basic Model Management

```python
from ai_models import get_model, has_vision, has_prompt_caching

# Get model specifications
model = get_model("claude-sonnet-4-5")
print(f"Context: {model.metadata.context_window_input:,} tokens")
print(f"Cost: ${model.pricing.input_per_1m}/M input tokens")

# Calculate costs with caching
cost = model.calculate_cost(
    input_tokens=10_000,
    output_tokens=2_000,
    cached_input_tokens=5_000  # 90% discount on cached tokens
)

# Validate capabilities before API calls
if has_vision("gpt-4o"):
    process_image()
```

### Finding Budget-Friendly Models

```python
from ai_models import ModelRegistry

budget_models = ModelRegistry.filter_by_cost_tier("budget")
# Returns: Haiku 4.5, GPT-4o mini, Gemini Flash
```

**Advanced Examples:** [API Documentation](./docs/api-examples.md) | [Production Architecture](./examples/epic-categorization/)

---

## ğŸ¤– Model Selection Guide

Choose the right model for your workload:

| Model | Input/Output (per 1M) | Context | Best For | Our Usage |
|-------|----------------------|---------|----------|-----------|
| **Claude Haiku 4.5** | $1/$5 | 200K | High-volume classification | 70% |
| **Claude Sonnet 4.5** | $3/$15 | 200K | Production workhorse | 25% |
| **Claude Opus 4.1** | $15/$75 | 200K | High-stakes decisions | 5% |
| **GPT-4o** | $2.5/$10 | 128K | Multimodal, function calling | Specific |
| **GPT-4o mini** | $0.15/$0.60 | 128K | Budget alternative | Budget |
| **Gemini 2.5 Pro** | $1.25/$5 | 2M | Massive context analysis | Large docs |
| **Gemini 2.5 Flash** | $0.075/$0.30 | 1M | Speed-critical apps | Real-time |

*Pricing verified October 2025. See [MODEL_OPTIMIZATION_GUIDE.md](./MODEL_OPTIMIZATION_GUIDE.md) for detailed comparison.*

### Cost Optimization Pattern

**Don't use one model for everything.** Intelligent routing saves 99.7%:

```
Keyword Filter (free) â†’ 70% resolved
    â†“
Haiku ($0.0003/signal) â†’ 25% resolved
    â†“
Sonnet ($0.002/signal) â†’ 4.5% resolved
    â†“
Opus ($0.015/signal) â†’ 0.5% resolved

Average: $0.001/signal (vs $0.015 naive approach)
```

[â†’ Implementation Guide](./docs/cost-optimization.md)

---

## ğŸ“ Repository Structure

```
PM-Prompt-Patterns/
â”œâ”€â”€ prompts/              # Production-ready prompts by category
â”‚   â”œâ”€â”€ analytics/        # Monitoring, reporting, investigation (MECE)
â”‚   â”œâ”€â”€ product-strategy/ # Roadmapping, prioritization
â”‚   â””â”€â”€ technical-docs/   # API docs, CLAUDE.md generation
â”œâ”€â”€ ai_models/            # Python model management system
â”‚   â”œâ”€â”€ registry.py       # YAML-based model registry
â”‚   â”œâ”€â”€ pricing.py        # Cost calculation with caching
â”‚   â””â”€â”€ definitions/      # Model specs (Anthropic, OpenAI, Google)
â”œâ”€â”€ examples/             # Complete production systems
â”‚   â””â”€â”€ epic-categorization/  # 95% accuracy, $0.001/signal
â”œâ”€â”€ tests/                # 97 tests for model validation
â””â”€â”€ docs/                 # Deep-dive guides
```

**Navigate by Experience:**
- ğŸŸ¢ **Beginner:** [prompts/](./prompts/) basic patterns
- ğŸŸ¡ **Intermediate:** [MODEL_OPTIMIZATION_GUIDE.md](./MODEL_OPTIMIZATION_GUIDE.md)
- ğŸ”´ **Advanced:** [examples/epic-categorization/](./examples/epic-categorization/)

---

## ğŸ” Security & Compliance

**Security Scanning:** Multi-layer (Bandit, Safety, pip-audit, Semgrep, TruffleHog)
**Dependency Updates:** Automated weekly via Dependabot
**API Key Management:** Environment variables only, never committed
**Vulnerability Reporting:** Open GitHub issue with `security` label

**CI/CD Status:** All security scans pass. See [workflows](./.github/workflows/) for details.

---

## ğŸ“ˆ Performance & Scalability

**Proven Metrics:**
- **Throughput:** 5,000+ signals/week in production
- **Accuracy:** 95% (vs 85% manual baseline)
- **Cost Efficiency:** $0.001/signal average (99.7% reduction)
- **Latency:** <2s p95 with model cascading
- **Cache Hit Rate:** 95% on repeat patterns

**Scalability:**
- Batch processing: 50-100 signals/batch for 92% cost reduction
- Prompt caching: 90% discount on cached tokens
- Model registry: LRU-cached for <1ms lookups

[â†’ Benchmarks & Architecture](./docs/performance.md)

---

## ğŸ¤ Support & Ownership

**Team:** Product Infrastructure
**Issues:** [GitHub Issues](https://github.com/awoods187/PM-Prompt-Patterns/issues)
**Contributions:** [CONTRIBUTING.md](./CONTRIBUTING.md) | [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md)
**Response Time:** Best effort (open source project)

**Getting Help:**
1. Check [docs/](./docs/) for guides
2. Search existing issues
3. Open new issue with reproduction steps

---

## ğŸ“… Maintenance Status

| Section | Update Frequency | Next Update |
|---------|-----------------|-------------|
| Model Pricing | Monthly | Nov 2025 |
| Model Specs | As released | Ongoing |
| Security Scans | Weekly (automated) | Continuous |
| Dependencies | Weekly (Dependabot) | Continuous |
| Prompts | Ad-hoc | As contributed |

**Deprecation Policy:** 90-day notice for breaking changes. See [CHANGELOG.md](./CHANGELOG.md).

---

## ğŸ“ Learning Resources

**Fundamentals** (Start here):
1. [Prompt Design Principles](./PROMPT_DESIGN_PRINCIPLES.md) - Core patterns
2. [Model Optimization Guide](./MODEL_OPTIMIZATION_GUIDE.md) - Provider techniques
3. [Cost Optimization](./docs/cost-optimization.md) - ROI strategies

**Production Systems:**
- [Epic Categorization Example](./examples/epic-categorization/) - End-to-end architecture
- [Quality Evaluation](./docs/quality-evaluation.md) - Testing methodology
- [Meta-Prompting](./templates/meta-prompting.md) - Iterative improvement

**Migration:**
- [Old â†’ New System](./MIGRATION_GUIDE.md) - Upgrade from legacy registry

[â†’ Full Learning Path](./docs/learning-path.md)

---

## ğŸ¤ Contributing

We welcome production-tested contributions with real metrics:

**Quality Bar:**
- âœ… Quantified results from actual usage (no toy examples)
- âœ… Before/after metrics (accuracy, cost, latency)
- âœ… Failure modes documented
- âœ… Tests included (97 existing tests for reference)

**Process:**
1. Review [CONTRIBUTING.md](./CONTRIBUTING.md)
2. Sign commits with DCO: `git commit -s`
3. Submit PR using [template](./.github/pull_request_template.md)

**Recognition:** All contributors listed in [ATTRIBUTION.md](./ATTRIBUTION.md).

---

## ğŸ“„ License

**MIT License** - Commercial use, modification, distribution allowed. No warranty.

**TL;DR:** Use freely in commercial products, no attribution required (appreciated).

**Details:** [LICENSE](./LICENSE) | [LICENSE_FAQ.md](./LICENSE_FAQ.md) | [CONTENT_LICENSE.md](./CONTENT_LICENSE.md)

---

## ğŸ”— Quick Links

**Getting Started:** [Design Principles](./PROMPT_DESIGN_PRINCIPLES.md) â†’ [Model Guide](./MODEL_OPTIMIZATION_GUIDE.md) â†’ [Examples](./examples/)
**Developer API:** [ai_models Package](./docs/api-examples.md) | [Migration Guide](./MIGRATION_GUIDE.md)
**CI/CD:** [Workflows](./.github/workflows/) | [Run Tests](./scripts/run_tests.sh)
**Model Registry:** [Definitions](./ai_models/definitions/) | [Pricing](./ai_models/pricing.py)

---

**Note:** All examples genericized for public sharing. No proprietary data included. Metrics approximate and rounded for privacy.
